import spacy
from collections import Counter
import os

def get_adjective_analysis(file_path, top_n=10):
    """
    Reads a text file, processes it with spaCy, and calculates the frequency 
    distribution of adjective lemmas (base forms).

    NOTE: This requires the 'en_core_web_sm' model.
    Run: python -m spacy download en_core_web_sm

    Args:
        file_path (str): The path to the text file to analyze.
        top_n (int): The number of most frequent adjective lemmas to display.
    """
    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        return

    try:
        # Load the English language model
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("\n--- SPACY ERROR ---")
        print("Required spaCy model 'en_core_web_sm' not found.")
        print("Please run 'python -m spacy download en_core_web_sm' in your terminal.")
        print("-------------------")
        return

    # 1. Read the text file
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    if not text.strip():
        print("Error: The file is empty or contains only whitespace.")
        return

    # 2. Process the text with spaCy
    print(f"Processing text from: {file_path}...")
    doc = nlp(text)

    # 3. Identify and collect adjective lemmas
    adjective_lemmas = []
    
    # Iterate over the tokens and check the POS tag
    for token in doc:
        # Check if the token is an adjective
        if token.pos_ == "ADJ":
            # Use the lemma (base form) for counting
            adjective_lemmas.append(token.lemma_.lower())

    if not adjective_lemmas:
        print("No adjectives found in the text.")
        return

    # 4. Calculate the frequency of each adjective lemma
    adjective_counts = Counter(adjective_lemmas)
    total_adjectives = len(adjective_lemmas)

    # 5. Output the results
    print("\n--- Adjective Frequency Analysis ---")
    print(f"Total Adjectives Found: {total_adjectives}")
    print(f"Total Unique Adjective Lemmas Found: {len(adjective_counts)}")

    print(f"\n--- Top {top_n} Most Frequent Adjective Lemmas ---")
    
    top_adjectives = adjective_counts.most_common(top_n)

    if top_adjectives:
        print(f"{'Lemma':<15} {'Count':<8} {'Frequency (%)':<15}")
        print("-" * 38)
        
        # Calculate the total count of all adjectives for percentage calculation
        for lemma, count in top_adjectives:
            frequency_percent = (count / total_adjectives) * 100
            print(f"{lemma:<15} {count:<8} {frequency_percent:>.2f}%")
    else:
        print("Could not generate frequency table for adjectives.")

# --- Example Usage ---
# NOTE: Replace with the actual file path generated by your previous cleaning script.
FILE_TO_ANALYZE = 'kay-mann-at-enron-com-combined.txt' 

# Run the analysis
get_adjective_analysis(FILE_TO_ANALYZE, top_n=10)