import spacy
from collections import Counter
import os

def get_top_verbs(file_path):
    """
    Reads a text file, processes it with spaCy, and identifies the five most
    frequently used verb lemmas (base forms).

    NOTE: This requires the 'en_core_web_sm' model to be downloaded.
    Run: python -m spacy download en_core_web_sm

    Args:
        file_path (str): The path to the text file to analyze.
    """
    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        return

    try:
        # Load the English language model
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("\n--- SPACY ERROR ---")
        print("Required spaCy model 'en_core_web_sm' not found.")
        print("Please run 'python -m spacy download en_core_web_sm' in your terminal.")
        print("-------------------")
        return

    # 1. Read the text file
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    if not text.strip():
        print("Error: The file is empty or contains only whitespace.")
        return

    # 2. Process the text with spaCy
    print(f"Processing text from: {file_path}...")
    doc = nlp(text)

    # 3. Identify and collect verb lemmas
    verb_lemmas = []
    for token in doc:
        # Check if the token is a verb and not a stop word (optional, but cleaner)
        if token.pos_ == "VERB" and not token.is_stop:
            # We use the lemma (base form) for accurate counting
            verb_lemmas.append(token.lemma_.lower())

    if not verb_lemmas:
        print("No verbs found in the text.")
        return

    # 4. Count the frequency of each lemma
    verb_counts = Counter(verb_lemmas)

    # 5. Get the five most common verbs
    top_5_verbs = verb_counts.most_common(5)

    # 6. Output the results
    print("\n--- Five Most Frequently Used Verb Lemmas ---")
    if top_5_verbs:
        for lemma, count in top_5_verbs:
            print(f"- '{lemma}': {count} occurrences")
    else:
        print("Analysis found no verbs.")
    print(f"\nTotal unique verb lemmas found: {len(verb_counts)}")

# --- Example Usage ---
# NOTE: Replace with the actual file path generated by your previous cleaning script.
FILE_TO_ANALYZE = 'vince-kaminski-at-enron-com-combined.txt' 

# Run the analysis
# NOTE: You must have spaCy installed and the 'en_core_web_sm' model downloaded to run this successfully.
get_top_verbs(FILE_TO_ANALYZE)