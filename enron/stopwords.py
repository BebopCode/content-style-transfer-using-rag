import spacy
from collections import Counter
import os

def get_stop_word_frequency(file_path, top_n=10):
    """
    Reads a text file, processes it with spaCy, and calculates the frequency 
    distribution of all stop words.

    NOTE: This requires the 'en_core_web_sm' model.
    Run: python -m spacy download en_core_web_sm

    Args:
        file_path (str): The path to the text file to analyze.
        top_n (int): The number of most frequent stop words to display.
    """
    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        return

    try:
        # Load the English language model
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("\n--- SPACY ERROR ---")
        print("Required spaCy model 'en_core_web_sm' not found.")
        print("Please run 'python -m spacy download en_core_web_sm' in your terminal.")
        print("-------------------")
        return

    # 1. Read the text file
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    if not text.strip():
        print("Error: The file is empty or contains only whitespace.")
        return

    # 2. Process the text with spaCy
    print(f"Processing text from: {file_path}...")
    doc = nlp(text)

    # 3. Identify and collect stop words
    stop_words_list = []
    
    # We iterate over the tokens and use the built-in is_stop attribute
    for token in doc:
        # We also check if the token is mostly letters to avoid counting empty strings or random characters
        if token.is_stop and token.is_alpha:
            stop_words_list.append(token.text.lower())

    if not stop_words_list:
        print("No stop words found in the text.")
        return

    # 4. Calculate the frequency of each stop word
    stop_word_counts = Counter(stop_words_list)
    total_stop_words = len(stop_words_list)

    # 5. Output the results
    print("\n--- Stop Word Frequency Analysis ---")
    print(f"Total Stop Words Found: {total_stop_words}")
    print(f"Total Unique Stop Words Found: {len(stop_word_counts)}")

    print(f"\n--- Top {top_n} Most Frequent Stop Words ---")
    
    top_stop_words = stop_word_counts.most_common(top_n)

    if top_stop_words:
        # Calculate the total count of the top N for percentage calculation
        top_n_count = sum(count for _, count in top_stop_words)

        print(f"{'Word':<10} {'Count':<8} {'Frequency (%)':<15}")
        print("-" * 33)
        for word, count in top_stop_words:
            frequency_percent = (count / total_stop_words) * 100
            print(f"{word:<10} {count:<8} {frequency_percent:>.2f}%")
    else:
        print("Could not generate frequency table.")

# --- Example Usage ---
# NOTE: Replace with the actual file path generated by your previous cleaning script.
FILE_TO_ANALYZE = 'vince-kaminski-at-enron-com-combined.txt' 

# Run the analysis
get_stop_word_frequency(FILE_TO_ANALYZE, top_n=10)